{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"e017_add_same_area_flg_and_log_feat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アイディア、以下の特徴量を実装する\n",
    "# - 今の部屋と、前の部屋のwid_cd、ken_cd、lrg_cd、sml_cdが一緒かどうか\n",
    "# - logの中で、宿の情報の統計値（平均、最大、最小、分散、中央値）をとる\n",
    "#   total_room_cnt、wireless_lan_flg、yad_type、onsen_flg、kd_stn_5min、kd_bch_5min、kd_slp_5min、kd_conv_walk_5min\n",
    "# - logの中で、以下の最頻値をとる\n",
    "#   wid_cd、ken_cd、lrg_cd、sml_cd\n",
    "# - logの中で、areaが一緒だったら1, 異なっていたら0として、その統計値（平均、最大、最小、分散、中央値）をとる\n",
    "# - session lengthが2以上の場合は、該当の宿がreverse_seq_no(max_seq_no - seq_no)を追加し、reverse_seq_noが偶数か奇数かのフラグを立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    OUTPUT_DIR = f\"../saved_data/{NOTEBOOK_NAME}\"\n",
    "    SEED = 33\n",
    "    TARGET_COL = \"reserve\"\n",
    "\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = pd.read_csv(\"../data/train_log.csv\")\n",
    "train_label = pd.read_csv(\"../data/train_label.csv\")\n",
    "\n",
    "test_log = pd.read_csv(\"../data/test_log.csv\")\n",
    "test_session = pd.read_csv(\"../data/test_session.csv\")\n",
    "\n",
    "yado = pd.read_csv(\"../data/yado.csv\")\n",
    "\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "# # image_embeddings = pd.read_parquet(\"../data/image_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\n",
    "    f\"../saved_data/e016_make_train_popular_base/e016_make_train_popular_base_merged_train.pkl\"\n",
    ")\n",
    "\n",
    "test = pd.read_pickle(\n",
    "    f\"../saved_data/e016_make_train_popular_base/e016_make_train_popular_base_merged_test.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# train = train.sample(10000, random_state=Config.SEED).reset_index(drop=True)\n",
    "# test = test.sample(10000, random_state=Config.SEED).reset_index(drop=True)\n",
    "# train_log = train_log.sample(10000, random_state=Config.SEED).reset_index(drop=True)\n",
    "# test_log = test_log.sample(10000, random_state=Config.SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug用\n",
    "# train = train.sample(10000, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ひとつ前のlogのyad_idの値を取得\n",
    "# train[\"previous_1_yad_no\"] = train[\"logged_yad_no_list\"].apply(lambda x: x[-1])\n",
    "# test[\"previous_1_yad_no\"] = test[\"logged_yad_no_list\"].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sessionの情報を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_length(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionの長さを追加する\n",
    "    \"\"\"\n",
    "    train_session_id_cnt_dict = (\n",
    "        train_log.groupby(\"session_id\")[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "    test_session_id_cnt_dict = (\n",
    "        test_log.groupby(\"session_id\")[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "\n",
    "    train[\"session_length\"] = train[\"session_id\"].map(train_session_id_cnt_dict)\n",
    "    test[\"session_length\"] = test[\"session_id\"].map(test_session_id_cnt_dict)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_length(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_yado_nunique(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionの中で登場したyadoの数を追加する\n",
    "    \"\"\"\n",
    "    train_session_yado_cnt_dict = (\n",
    "        train_log.groupby(\"session_id\")[\"yad_no\"].nunique().to_dict()\n",
    "    )\n",
    "    test_session_yado_cnt_dict = (\n",
    "        test_log.groupby(\"session_id\")[\"yad_no\"].nunique().to_dict()\n",
    "    )\n",
    "\n",
    "    train[\"session_yado_nunique\"] = train[\"session_id\"].map(train_session_yado_cnt_dict)\n",
    "    test[\"session_yado_nunique\"] = test[\"session_id\"].map(test_session_yado_cnt_dict)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_yado_nunique(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_yado_cnt(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionとyadごとに、yad_noが登場した回数を数える\n",
    "    \"\"\"\n",
    "    train_session_yado_cnt_dict = (\n",
    "        train_log.groupby([\"session_id\", \"yad_no\"])[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "    test_session_yado_cnt_dict = (\n",
    "        test_log.groupby([\"session_id\", \"yad_no\"])[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "\n",
    "    train[\"session_yado_cnt\"] = train.apply(\n",
    "        lambda x: train_session_yado_cnt_dict[(x[\"session_id\"], x[\"yad_no\"])]\n",
    "        if (x[\"session_id\"], x[\"yad_no\"]) in train_session_yado_cnt_dict\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "    test[\"session_yado_cnt\"] = test.apply(\n",
    "        lambda x: test_session_yado_cnt_dict[(x[\"session_id\"], x[\"yad_no\"])]\n",
    "        if (x[\"session_id\"], x[\"yad_no\"]) in test_session_yado_cnt_dict\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_yado_cnt(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正解のyadoと1つ前のyadoの情報の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一番直近に見たyad_noを追加\n",
    "train_previous_1_yad_no_dict = (\n",
    "    train_log.groupby(\"session_id\")[\"yad_no\"].apply(lambda x: list(x)[-1]).to_dict()\n",
    ")\n",
    "test_previous_1_yad_no_dict = (\n",
    "    test_log.groupby(\"session_id\")[\"yad_no\"].apply(lambda x: list(x)[-1]).to_dict()\n",
    ")\n",
    "\n",
    "train[\"previous_1_yad_no\"] = train[\"session_id\"].map(train_previous_1_yad_no_dict)\n",
    "test[\"previous_1_yad_no\"] = test[\"session_id\"].map(test_previous_1_yad_no_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解のyado情報の追加\n",
    "train = pd.merge(\n",
    "    train,\n",
    "    yado.add_prefix(\"now_\"),\n",
    "    left_on=\"yad_no\",\n",
    "    right_on=\"now_yad_no\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 1つ前のyado情報の追加\n",
    "train = pd.merge(\n",
    "    train,\n",
    "    yado.add_prefix(\"previous_1_\"),\n",
    "    on=\"previous_1_yad_no\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解のyado情報の追加\n",
    "test = pd.merge(\n",
    "    test,\n",
    "    yado.add_prefix(\"now_\"),\n",
    "    left_on=\"yad_no\",\n",
    "    right_on=\"now_yad_no\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 1つ前のyado情報の追加\n",
    "test = pd.merge(\n",
    "    test,\n",
    "    yado.add_prefix(\"previous_1_\"),\n",
    "    on=\"previous_1_yad_no\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今の部屋と、前の部屋のwid_cd、ken_cd、lrg_cd、sml_cdが一緒かどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_wid_cd_now_previous_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow_wid_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_1_wid_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_ken_cd_now_previous_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow_ken_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_1_ken_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_lrg_cd_now_previous_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow_lrg_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_1_lrg_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train[\"same_wid_cd_now_previous_1\"] = train[\"now_wid_cd\"] == train[\"previous_1_wid_cd\"]\n",
    "train[\"same_ken_cd_now_previous_1\"] = train[\"now_ken_cd\"] == train[\"previous_1_ken_cd\"]\n",
    "train[\"same_lrg_cd_now_previous_1\"] = train[\"now_lrg_cd\"] == train[\"previous_1_lrg_cd\"]\n",
    "train[\"same_sml_cd_now_previous_1\"] = train[\"now_sml_cd\"] == train[\"previous_1_sml_cd\"]\n",
    "\n",
    "test[\"same_wid_cd_now_previous_1\"] = test[\"now_wid_cd\"] == test[\"previous_1_wid_cd\"]\n",
    "test[\"same_ken_cd_now_previous_1\"] = test[\"now_ken_cd\"] == test[\"previous_1_ken_cd\"]\n",
    "test[\"same_lrg_cd_now_previous_1\"] = test[\"now_lrg_cd\"] == test[\"previous_1_lrg_cd\"]\n",
    "test[\"same_sml_cd_now_previous_1\"] = test[\"now_sml_cd\"] == test[\"previous_1_sml_cd\"]\n",
    "\n",
    "same_area_feat = [\n",
    "    \"same_wid_cd_now_previous_1\",\n",
    "    \"same_ken_cd_now_previous_1\",\n",
    "    \"same_lrg_cd_now_previous_1\",\n",
    "    \"same_sml_cd_now_previous_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、宿の情報の統計値（平均、最大、最小、分散、中央値）をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yad_statistic_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\n",
    "        \"yad_type\",\n",
    "        \"total_room_cnt\",\n",
    "        \"wireless_lan_flg\",\n",
    "        \"onsen_flg\",\n",
    "        \"kd_stn_5min\",\n",
    "        \"kd_bch_5min\",\n",
    "        \"kd_slp_5min\",\n",
    "        \"kd_conv_walk_5min\",\n",
    "    ]\n",
    "\n",
    "    agg_ways = [\"mean\", \"max\", \"min\", \"std\", \"median\"]\n",
    "\n",
    "    for yad_col in use_cols:\n",
    "        agg_df = (\n",
    "            log_with_yad.groupby(\"session_id\")[yad_col]\n",
    "            .agg(agg_ways)\n",
    "            .add_prefix(f\"{yad_col}_\")\n",
    "        )\n",
    "        session_df = pd.merge(session_df, agg_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    agg_col_name = [\n",
    "        f\"{yad_col}_{agg_way}\" for yad_col in use_cols for agg_way in agg_ways\n",
    "    ]\n",
    "\n",
    "    return session_df, agg_col_name\n",
    "\n",
    "\n",
    "train, yado_agg_col = add_yad_statistic_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_statistic_from_log(test_log, test, yado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、wid_cd、ken_cd、lrg_cd、sml_cdの最頻値をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: あんまり寄与していないなら遅いので削除する\n",
    "def add_yad_area_mode_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\"wid_cd\", \"ken_cd\", \"lrg_cd\", \"sml_cd\"]\n",
    "\n",
    "    for area_col in use_cols:\n",
    "        agg_dict = (\n",
    "            log_with_yad.groupby(\"session_id\")[area_col]\n",
    "            .agg(lambda x: x.value_counts().index[0])\n",
    "            .to_dict()\n",
    "        )\n",
    "        session_df[f\"{area_col}_mode\"] = session_df[\"session_id\"].map(agg_dict)\n",
    "\n",
    "    out_cols = [f\"{col}_mode\" for col in use_cols]\n",
    "\n",
    "    return session_df, out_cols\n",
    "\n",
    "\n",
    "train, yad_area_mode_cols = add_yad_area_mode_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_area_mode_from_log(train_log, test, yado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、wid_cd、ken_cd、lrg_cd、sml_cdのnuniqueをとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yad_area_nunique_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\"wid_cd\", \"ken_cd\", \"lrg_cd\", \"sml_cd\"]\n",
    "\n",
    "    for area_col in use_cols:\n",
    "        agg_df = log_with_yad.groupby(\"session_id\")[area_col].nunique()\n",
    "        session_df = session_df.merge(agg_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    out_cols = [f\"{col}_nunique\" for col in use_cols]\n",
    "\n",
    "    return session_df, out_cols\n",
    "\n",
    "\n",
    "train, yad_area_nunique = add_yad_area_nunique_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_area_nunique_from_log(train_log, test, yado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、areaが一緒だったら1, 異なっていたら0として、その統計値（平均、最大、最小、分散、中央値）をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_urouro(area_cd_list: list):\n",
    "    if len(area_cd_list) == 1:\n",
    "        return None, None, None, None\n",
    "    else:\n",
    "        urouro = []\n",
    "        previous_area_cd = \"\"\n",
    "        for idx, area_cd in enumerate(area_cd_list):\n",
    "            if idx == 0:\n",
    "                previous_area_cd = area_cd\n",
    "                continue\n",
    "            else:\n",
    "                if previous_area_cd != area_cd:\n",
    "                    urouro.append(1)\n",
    "                else:\n",
    "                    urouro.append(0)\n",
    "                previous_area_cd = area_cd\n",
    "        return np.mean(urouro), np.max(urouro), np.min(urouro), np.std(urouro)\n",
    "\n",
    "\n",
    "def add_yad_area_urouro_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\"wid_cd\", \"ken_cd\", \"lrg_cd\", \"sml_cd\"]\n",
    "\n",
    "    for area_col in use_cols:\n",
    "        session_id_area_dict = (\n",
    "            log_with_yad.groupby(\"session_id\")[area_col].apply(list).to_dict()\n",
    "        )\n",
    "        session_df[f\"{area_col}_list\"] = session_df[\"session_id\"].map(\n",
    "            session_id_area_dict\n",
    "        )\n",
    "        urouro_df = session_df[f\"{area_col}_list\"].apply(calc_urouro)\n",
    "        urouro_df = pd.DataFrame(\n",
    "            urouro_df.tolist(),\n",
    "            columns=[\n",
    "                f\"{area_col}_urouro_mean\",\n",
    "                f\"{area_col}_urouro_max\",\n",
    "                f\"{area_col}_urouro_min\",\n",
    "                f\"{area_col}_urouro_std\",\n",
    "            ],\n",
    "        )\n",
    "        session_df = pd.concat([session_df, urouro_df], axis=1)\n",
    "        session_df.drop(columns=[f\"{area_col}_list\"], inplace=True)\n",
    "\n",
    "    out_cols = [\n",
    "        f\"{area}_{stat}\"\n",
    "        for stat in [\"urouro_mean\", \"urouro_max\", \"urouro_min\", \"urouro_std\"]\n",
    "        for area in use_cols\n",
    "    ]\n",
    "    return session_df, out_cols\n",
    "\n",
    "\n",
    "train, yad_area_urouro_from_log = add_yad_area_urouro_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_area_urouro_from_log(test_log, test, yado)\n",
    "# 約8分かかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session lengthが2以上の場合は、該当の宿がreverse_seq_no(max_seq_no - seq_no)を追加し、reverse_seq_noが偶数か奇数かのフラグを立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reverse_seq_no(log_df: pd.DataFrame, session_df: pd.DataFrame):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "\n",
    "    # session_dfにseq_noを結合する\n",
    "    log_df_no_dup = log_df[[\"session_id\", \"seq_no\"]].drop_duplicates()\n",
    "    session_df = session_df.merge(log_df_no_dup, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # sessionごとに最大のseq_noを結合する\n",
    "    log_max_seq_no = log_df.groupby(\"session_id\")[\"seq_no\"].max()\n",
    "    log_max_seq_no.name = \"max_seq_no\"\n",
    "    session_df = session_df.merge(log_max_seq_no, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # sessionの最大のseq_noの差分を取る(そのセッションが最後から何番目か？)\n",
    "    session_df[\"reverse_seq_no\"] = session_df[\"max_seq_no\"] - session_df[\"seq_no\"]\n",
    "    session_df[\"is_reverse_seq_no_odd\"] = session_df[\"reverse_seq_no\"] % 2\n",
    "\n",
    "    session_df.drop(columns=[\"seq_no\", \"max_seq_no\"], inplace=True)\n",
    "\n",
    "    return session_df\n",
    "\n",
    "\n",
    "train = add_reverse_seq_no(train_log, train)\n",
    "test = add_reverse_seq_no(test_log, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_seq_no_col = [\"reverse_seq_no\", \"is_reverse_seq_no_odd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ型の変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_colの定義\n",
    "base_categorical_cols = [\n",
    "    \"yad_no\",\n",
    "    \"yad_type\",\n",
    "    \"wid_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"ken_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"lrg_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"sml_cd\",\n",
    "]\n",
    "now_yado_categorical_cols = [f\"now_{col}\" for col in base_categorical_cols]\n",
    "previous_1_yado_categorical_cols = [\n",
    "    f\"previous_1_{col}\" for col in base_categorical_cols\n",
    "]\n",
    "\n",
    "categorical_cols = (\n",
    "    now_yado_categorical_cols\n",
    "    + previous_1_yado_categorical_cols\n",
    "    + yad_area_mode_cols  # logの宿のareaの最頻値\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['session_id',\n",
       " 'yad_no',\n",
       " 'reserve',\n",
       " 'session_length',\n",
       " 'session_yado_nunique',\n",
       " 'session_yado_cnt',\n",
       " 'previous_1_yad_no',\n",
       " 'now_yad_no',\n",
       " 'now_yad_type',\n",
       " 'now_total_room_cnt',\n",
       " 'now_wireless_lan_flg',\n",
       " 'now_onsen_flg',\n",
       " 'now_kd_stn_5min',\n",
       " 'now_kd_bch_5min',\n",
       " 'now_kd_slp_5min',\n",
       " 'now_kd_conv_walk_5min',\n",
       " 'now_wid_cd',\n",
       " 'now_ken_cd',\n",
       " 'now_lrg_cd',\n",
       " 'now_sml_cd',\n",
       " 'previous_1_yad_type',\n",
       " 'previous_1_total_room_cnt',\n",
       " 'previous_1_wireless_lan_flg',\n",
       " 'previous_1_onsen_flg',\n",
       " 'previous_1_kd_stn_5min',\n",
       " 'previous_1_kd_bch_5min',\n",
       " 'previous_1_kd_slp_5min',\n",
       " 'previous_1_kd_conv_walk_5min',\n",
       " 'previous_1_wid_cd',\n",
       " 'previous_1_ken_cd',\n",
       " 'previous_1_lrg_cd',\n",
       " 'previous_1_sml_cd',\n",
       " 'same_wid_cd_now_previous_1',\n",
       " 'same_ken_cd_now_previous_1',\n",
       " 'same_lrg_cd_now_previous_1',\n",
       " 'same_sml_cd_now_previous_1',\n",
       " 'yad_type_mean',\n",
       " 'yad_type_max',\n",
       " 'yad_type_min',\n",
       " 'yad_type_std',\n",
       " 'yad_type_median',\n",
       " 'total_room_cnt_mean',\n",
       " 'total_room_cnt_max',\n",
       " 'total_room_cnt_min',\n",
       " 'total_room_cnt_std',\n",
       " 'total_room_cnt_median',\n",
       " 'wireless_lan_flg_mean',\n",
       " 'wireless_lan_flg_max',\n",
       " 'wireless_lan_flg_min',\n",
       " 'wireless_lan_flg_std',\n",
       " 'wireless_lan_flg_median',\n",
       " 'onsen_flg_mean',\n",
       " 'onsen_flg_max',\n",
       " 'onsen_flg_min',\n",
       " 'onsen_flg_std',\n",
       " 'onsen_flg_median',\n",
       " 'kd_stn_5min_mean',\n",
       " 'kd_stn_5min_max',\n",
       " 'kd_stn_5min_min',\n",
       " 'kd_stn_5min_std',\n",
       " 'kd_stn_5min_median',\n",
       " 'kd_bch_5min_mean',\n",
       " 'kd_bch_5min_max',\n",
       " 'kd_bch_5min_min',\n",
       " 'kd_bch_5min_std',\n",
       " 'kd_bch_5min_median',\n",
       " 'kd_slp_5min_mean',\n",
       " 'kd_slp_5min_max',\n",
       " 'kd_slp_5min_min',\n",
       " 'kd_slp_5min_std',\n",
       " 'kd_slp_5min_median',\n",
       " 'kd_conv_walk_5min_mean',\n",
       " 'kd_conv_walk_5min_max',\n",
       " 'kd_conv_walk_5min_min',\n",
       " 'kd_conv_walk_5min_std',\n",
       " 'kd_conv_walk_5min_median',\n",
       " 'wid_cd_mode',\n",
       " 'ken_cd_mode',\n",
       " 'lrg_cd_mode',\n",
       " 'sml_cd_mode',\n",
       " 'wid_cd_nunique',\n",
       " 'ken_cd_nunique',\n",
       " 'lrg_cd_nunique',\n",
       " 'sml_cd_nunique',\n",
       " 'wid_cd_urouro_mean',\n",
       " 'wid_cd_urouro_max',\n",
       " 'wid_cd_urouro_min',\n",
       " 'wid_cd_urouro_std',\n",
       " 'ken_cd_urouro_mean',\n",
       " 'ken_cd_urouro_max',\n",
       " 'ken_cd_urouro_min',\n",
       " 'ken_cd_urouro_std',\n",
       " 'lrg_cd_urouro_mean',\n",
       " 'lrg_cd_urouro_max',\n",
       " 'lrg_cd_urouro_min',\n",
       " 'lrg_cd_urouro_std',\n",
       " 'sml_cd_urouro_mean',\n",
       " 'sml_cd_urouro_max',\n",
       " 'sml_cd_urouro_min',\n",
       " 'sml_cd_urouro_std',\n",
       " 'reverse_seq_no',\n",
       " 'is_reverse_seq_no_odd']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_cols.remove(\"now_yad_no\")\n",
    "# categorical_cols.remove(\"previous_1_yad_no\")\n",
    "\n",
    "# # leak的な何かしらが起きているので、今は使わない\n",
    "# # TODO: 予測するyad_noは重要な情報だと思うため入れ方を考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    train[col] = train[col].astype(\"category\")\n",
    "    test[col] = test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross Validationの際に用いるfold(分割する際のグループ番号)を追加\n",
    "# FOLD_NUM = 5\n",
    "\n",
    "# # skf = StratifiedKFold(n_splits=FOLD_NUM, shuffle=True, random_state=42)\n",
    "# # for fold, (_, v_idx) in enumerate(\n",
    "# #     skf.split(train, pd.cut(train[\"reserve\"], bins=3, labels=[\"0\", \"0.5\", \"1\"]))\n",
    "# # ):\n",
    "# #     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# # TODO: yをsession_lengthにして試してみる\n",
    "# # sgkf = StratifiedGroupKFold(n_splits=FOLD_NUM, shuffle=True, random_state=Config.SEED)\n",
    "# # for fold, (_, v_idx) in enumerate(\n",
    "# #     sgkf.split(\n",
    "# #         X=train,\n",
    "# #         y=train[\"reserve\"],\n",
    "# #         groups=train[\"session_id\"],\n",
    "# #     )\n",
    "# # ):\n",
    "# #     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# gkf = GroupKFold(n_splits=FOLD_NUM)\n",
    "# for fold, (_, v_idx) in enumerate(\n",
    "#     gkf.split(\n",
    "#         X=train,\n",
    "#         groups=train[\"session_id\"],\n",
    "#     )\n",
    "# ):\n",
    "#     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# train[\"fold\"] = train[\"fold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e016にて、データ作成時にfoldを利用するように変更\n",
    "with open(\n",
    "    \"../saved_data/e016_make_train_popular_base/session_id_fold_dict.pkl\", \"rb\"\n",
    ") as f:\n",
    "    session_id_fold_dict = pickle.load(f)\n",
    "\n",
    "train[\"fold\"] = train[\"session_id\"].map(session_id_fold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold  reserve\n",
       "0     0          0.972202\n",
       "      1          0.027798\n",
       "1     0          0.972116\n",
       "      1          0.027884\n",
       "2     0          0.971962\n",
       "      1          0.028038\n",
       "3     0          0.972227\n",
       "      1          0.027773\n",
       "4     0          0.972166\n",
       "      1          0.027834\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"fold\")[\"reserve\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人気の宿情報、つまり予約された回数をfoldごとに付与する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_fold_df = train[[\"session_id\", \"fold\"]].drop_duplicates()\n",
    "session_id_fold_dict = dict(\n",
    "    zip(session_id_fold_df[\"session_id\"], session_id_fold_df[\"fold\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_popular_per_fold(\n",
    "    train: pd.DataFrame,\n",
    "    train_label: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    n_fold: int = FOLD_NUM,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    foldごとに、宿が予約された回数を計算する\n",
    "    NOTE: 人気情報を付与して0.42ぐらいかなかったら何か間違っていそう\n",
    "    \"\"\"\n",
    "    train = train.copy()\n",
    "    train_label_copy = train_label.copy()\n",
    "    test = test.copy()\n",
    "\n",
    "    # train_labelに対して、trainを用いてsession_idごとのfoldを付与\n",
    "    session_id_fold_df = train[[\"session_id\", \"fold\"]].drop_duplicates()\n",
    "    session_id_fold_dict = dict(\n",
    "        zip(session_id_fold_df[\"session_id\"], session_id_fold_df[\"fold\"])\n",
    "    )\n",
    "\n",
    "    # その後、train_labelを用いて、foldごとにreserveの合計を計算\n",
    "    train_label_copy[\"fold\"] = train_label_copy[\"session_id\"].map(session_id_fold_dict)\n",
    "\n",
    "    # 学習データへの人気宿情報の付与\n",
    "    for fold in range(n_fold):\n",
    "        train_out_of_fold_df = train_label_copy[train_label_copy[\"fold\"] != fold]\n",
    "\n",
    "        train_yad_no_cnt_per_fold_dict = (\n",
    "            train_out_of_fold_df.groupby(\"yad_no\")[\"yad_no\"].count()\n",
    "            / train_out_of_fold_df.shape[0]\n",
    "        ).to_dict()\n",
    "        train.loc[train[\"fold\"] == fold, \"popular_yado_rate_per_fold\"] = train.loc[\n",
    "            train[\"fold\"] == fold, \"yad_no\"\n",
    "        ].map(train_yad_no_cnt_per_fold_dict)\n",
    "\n",
    "    # テストデータへの人気宿情報の付与\n",
    "    test_yad_no_cnt_per_fold_dict = (\n",
    "        train_label_copy.groupby(\"yad_no\")[\"yad_no\"].count() / train_label_copy.shape[0]\n",
    "    ).to_dict()\n",
    "    test[\"popular_yado_rate_per_fold\"] = test[\"yad_no\"].map(\n",
    "        test_yad_no_cnt_per_fold_dict\n",
    "    )\n",
    "\n",
    "    # 学習・テスト共に、1度も登場しなかった宿は予約回数が0回となるため、0に置換\n",
    "    train[\"popular_yado_rate_per_fold\"] = train[\"popular_yado_rate_per_fold\"].fillna(0)\n",
    "    test[\"popular_yado_rate_per_fold\"] = test[\"popular_yado_rate_per_fold\"].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_popular_per_fold(train, train_label, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習とテストに使うデータを保存する\n",
    "train.to_pickle(\n",
    "    f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_train.pkl\",\n",
    ")\n",
    "\n",
    "test.to_pickle(\n",
    "    f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_test.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_colの定義\n",
    "base_numeric_col = [\n",
    "    \"total_room_cnt\",\n",
    "    \"wireless_lan_flg\",\n",
    "    \"onsen_flg\",\n",
    "    \"kd_stn_5min\",\n",
    "    \"kd_bch_5min\",\n",
    "    \"kd_slp_5min\",\n",
    "    \"kd_conv_walk_5min\",\n",
    "]\n",
    "\n",
    "now_yado_numeric_col = [f\"now_{col}\" for col in base_numeric_col]\n",
    "previous_1_yado_numeric_col = [f\"previous_1_{col}\" for col in base_numeric_col]\n",
    "\n",
    "# session系の特徴量\n",
    "session_numeric_col = [\n",
    "    \"session_length\",\n",
    "    \"session_yado_nunique\",\n",
    "    \"session_yado_cnt\",\n",
    "]\n",
    "\n",
    "per_fold_col = [\"popular_yado_rate_per_fold\"]\n",
    "\n",
    "# numeric_cols = now_yado_numeric_col + previous_1_yado_numeric_col\n",
    "# numeric_cols = now_yado_numeric_col\n",
    "numeric_cols = (\n",
    "    now_yado_numeric_col  # 今のyadoの情報\n",
    "    + previous_1_yado_numeric_col  # 一つ前のyadoの情報\n",
    "    + session_numeric_col  # session系の特徴量\n",
    "    + per_fold_col  # 人気の宿の情報\n",
    "    + same_area_feat  # 現在と一つ前のyadoのエリアが同じかどうか\n",
    "    + yado_agg_col  # logにおけるyadoの統計量\n",
    "    + yad_area_urouro_from_log  # logにおけるyadoのエリアのうろうろ度\n",
    "    + yad_area_nunique  # logにおけるyadoのエリアのユニーク数\n",
    "    + reverse_seq_no_col  # sessionの長さが2以上の場合は、該当の宿がreverse_seq_no(max_seq_no - seq_no)を追加し、reverse_seq_noが偶数か奇数かのフラグを立てる\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_col = numeric_cols + categorical_cols\n",
    "len(use_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_use_col.pkl\", \"wb\") as f:\n",
    "    pickle.dump(use_col, f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_numeric_cols.pkl\", \"wb\") as f:\n",
    "    pickle.dump(numeric_cols, f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_categorical_cols.pkl\", \"wb\") as f:\n",
    "    pickle.dump(categorical_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで用いるパラメーターを指定\n",
    "# ref: https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"max_depth\": -1,\n",
    "    # \"min_data_in_leaf\": 100, # 1つの葉に入る最小のデータ数\n",
    "    \"num_leaves\": 24,  # 2**max_depthより少し小さめにすると過学習を防げる\n",
    "    \"learning_rate\": 0.05,  # 1回のiterationで学習を進める割合、大きいと学習が早く終わる。小さいと学習は長いが高精度になりやすい。\n",
    "    \"bagging_freq\": 5,  # 指定した回数ごとにbaggingを行う\n",
    "    \"feature_fraction\": 0.9,  # 1回のiterationで利用する特徴量(列方向)の割合\n",
    "    \"bagging_fraction\": 0.8,  # 1回のiterationで利用するデータ(行方向)の割合\n",
    "    \"verbose\": -1,  # 出力するログレベルの変更、0はError(Warning)以上を表示\n",
    "    \"seed\": 42,  # ランダムシードの固定\n",
    "    \"lambda_l1\": 0.4,\n",
    "    \"lambda_l2\": 0.4,\n",
    "    \"importance_type\": \"gain\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     # 目的関数. これの意味で最小となるようなパラメータを探します.\n",
    "#     \"objective\": \"binary\",\n",
    "#     # 木の最大数\n",
    "#     \"n_estimators\": 10000,\n",
    "#     # 学習率. 小さいほどなめらかな決定境界が作られて性能向上に繋がる場合が多いです、\n",
    "#     # がそれだけ木を作るため学習に時間がかかります\n",
    "#     # 今回設定している 0.3 は比較的大きめの設定です\n",
    "#     \"learning_rate\": 0.3,\n",
    "#     # 特徴重要度計算のロジック(後述)\n",
    "#     \"importance_type\": \"gain\",\n",
    "#     \"random_state\": 510,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rmse(y_true, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ全体に対する推論結果を保存するobjectを作成\n",
    "oof = np.zeros((len(train)))\n",
    "\n",
    "# テストデータに対する推論、特徴量重要度(後述)を計算するために、モデルを保存するobjectを作成\n",
    "models = []\n",
    "\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(FOLD_NUM):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train[train[\"fold\"] != fold]\n",
    "    valid_fold = train[train[\"fold\"] == fold]\n",
    "\n",
    "    # X(説明変数)とy(目的変数)に分ける\n",
    "    X_train = train_fold.drop(Config.TARGET_COL, axis=1)\n",
    "    X_valid = valid_fold.drop(Config.TARGET_COL, axis=1)\n",
    "    y_train = train_fold[[Config.TARGET_COL]]\n",
    "    y_valid = valid_fold[[Config.TARGET_COL]]\n",
    "\n",
    "    # 利用する説明変数に限定する\n",
    "    X_train = X_train[use_col]\n",
    "    X_valid = X_valid[use_col]\n",
    "\n",
    "    # LightGBMが認識可能な形にデータセットを変換\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "    # モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=2000,  # 学習のiteration回数\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(100),\n",
    "        ],  # Early stopingの回数、binary_loglossが改善しないiterationが100回続いたら学習を止める\n",
    "    )\n",
    "\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "\n",
    "    # validデータに対する推論\n",
    "    y_valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "    # validデータに対する推論の性能を計算\n",
    "    # score = rmse(y_valid, y_valid_pred)\n",
    "    score = roc_auc_score(y_valid, y_valid_pred)\n",
    "\n",
    "    print(f\"fold {fold} Score: {score}\")\n",
    "\n",
    "    # oofに推論結果を保存\n",
    "    valid_idx = X_valid.index\n",
    "    oof[valid_idx] = y_valid_pred\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_oof.pkl\", \"wb\") as f:\n",
    "    pickle.dump(oof, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_models.pkl\", \"rb\") as f:\n",
    "    models = pickle.load(f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_oof.pkl\", \"rb\") as f:\n",
    "    oof = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_score = rmse(train[Config.TARGET_COL][oof != 0], oof[oof != 0])\n",
    "oof_score = roc_auc_score(train[Config.TARGET_COL][oof != 0], oof[oof != 0])\n",
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainをoofが0でないものだけに絞る\n",
    "def get_oof_pred_df(train: pd.DataFrame, oof: np.ndarray) -> pd.DataFrame:\n",
    "    train_for_calc_mapk = train.copy()\n",
    "    train_for_calc_mapk[\"oof\"] = oof\n",
    "    train_for_calc_mapk = train_for_calc_mapk[train_for_calc_mapk[\"oof\"] != 0]\n",
    "\n",
    "    # 推論順にsession_idとyad_noを並べる\n",
    "    oof_pred_yad = (\n",
    "        train_for_calc_mapk.sort_values([\"session_id\", \"oof\"], ascending=False)\n",
    "        .groupby(\"session_id\")[\"yad_no\"]\n",
    "        .apply(list)\n",
    "    ).to_dict()\n",
    "\n",
    "    # train_labelをoofの計算用に用意\n",
    "    train_label_for_calc_oof = train_label.copy()\n",
    "\n",
    "    # train_for_calc_mapkに付与\n",
    "    train_label_for_calc_oof[\"pred_yad_no_list\"] = train_label_for_calc_oof[\n",
    "        \"session_id\"\n",
    "    ].map(oof_pred_yad)\n",
    "\n",
    "    # oofが計算されていないsession_idは削除\n",
    "    train_label_for_calc_oof = train_label_for_calc_oof[\n",
    "        train_label_for_calc_oof[\"pred_yad_no_list\"].notnull()\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # 上位10件に限定\n",
    "    train_label_for_calc_oof[\"pred_yad_no_list_top10\"] = train_label_for_calc_oof[\n",
    "        \"pred_yad_no_list\"\n",
    "    ].apply(lambda x: x[:10])\n",
    "\n",
    "    # listをpd.Seriesに変換\n",
    "    oof_pred_df = train_label_for_calc_oof.set_index(\"session_id\")[\n",
    "        \"pred_yad_no_list_top10\"\n",
    "    ].apply(pd.Series)\n",
    "    oof_pred_df = oof_pred_df.rename(columns=lambda x: \"predict_\" + str(x))\n",
    "\n",
    "    # Nullの箇所はyad_no=0で保管し、全ての値をintに変換する\n",
    "    # TODO: 埋めるのは0で本当に良いのか考える\n",
    "    oof_pred_df = oof_pred_df.fillna(0).astype(int)\n",
    "\n",
    "    return oof_pred_df\n",
    "\n",
    "\n",
    "oof_pred_df = get_oof_pred_df(train, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label[train_label[\"session_id\"].isin(oof_pred_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single actual value.\n",
    "\n",
    "    Parameters:\n",
    "    actual : int\n",
    "        The actual value that is to be predicted\n",
    "    predicted : list\n",
    "        A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The average precision at k\n",
    "    \"\"\"\n",
    "    if actual in predicted[:k]:\n",
    "        return 1.0 / (predicted[:k].index(actual) + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k for lists of actual values and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    actual : list\n",
    "        A list of actual values that are to be predicted\n",
    "    predicted : list\n",
    "        A list of lists of predicted elements (order does matter in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The mean average precision at k\n",
    "    \"\"\"\n",
    "    return sum(apk(a, p, k) for a, p in zip(actual, predicted)) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPK (k=10) として計算\n",
    "sorted_train_label = (\n",
    "    train_label[train_label[\"session_id\"].isin(oof_pred_df.index)]\n",
    "    .sort_values(\"session_id\")[\"yad_no\"]\n",
    "    .values\n",
    ")\n",
    "\n",
    "assert len(sorted_train_label) == len(oof_pred_df)\n",
    "\n",
    "oof_mapk_score = mapk(\n",
    "    actual=sorted_train_label,\n",
    "    predicted=oof_pred_df.sort_index().values.tolist(),\n",
    "    k=10,\n",
    ")\n",
    "oof_mapk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度を列にもつDataFrameを作成\n",
    "feature_importances = [\n",
    "    model.feature_importance(importance_type=\"gain\") for model in models\n",
    "]\n",
    "feature_importances_df = pd.DataFrame(feature_importances, columns=use_col)\n",
    "\n",
    "# 表示する順番を指定、特徴量重要度の平均が大きい順に並ぶよう計算\n",
    "order = feature_importances_df.mean().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# 表示\n",
    "# fold毎の特徴量重要度のばらつきを見るために、箱ひげ図を利用\n",
    "sns.boxplot(data=feature_importances_df, orient=\"h\", order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testに対する推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test[use_col]) for model in models])\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0)  # axis=0 なので shape の `k` が潰れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = 0.02\n",
    "# bins = np.linspace(0, 1, 0.1)\n",
    "ax.hist(pred, density=True, alpha=0.5, label=\"Test\")\n",
    "ax.hist(oof, density=True, alpha=0.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP10に並び替え\n",
    "# session_idごとにpredが高いyadoのlistを取得\n",
    "pred_yad = (\n",
    "    test.sort_values([\"session_id\", \"pred\"], ascending=False)\n",
    "    .groupby(\"session_id\")[\"yad_no\"]\n",
    "    .apply(list)\n",
    ").to_dict()\n",
    "\n",
    "test_session[\"pred_yad_no_list\"] = test_session[\"session_id\"].map(pred_yad)\n",
    "\n",
    "# 上位10件に限定\n",
    "test_session[\"pred_yad_no_list_top10\"] = test_session[\"pred_yad_no_list\"].apply(\n",
    "    lambda x: x[:10]\n",
    ")\n",
    "\n",
    "# listをpd.Seriesに変換\n",
    "pred_yad_df = test_session[\"pred_yad_no_list_top10\"].apply(pd.Series)\n",
    "pred_yad_df = pred_yad_df.rename(columns=lambda x: \"predict_\" + str(x))\n",
    "\n",
    "print(pred_yad_df.isnull().sum())\n",
    "\n",
    "# Nullの箇所はyad_no=10095(一番人気)で保管し、全ての値をintに変換する\n",
    "# NOTE: 保管するのは本当に10095で良いのか考える\n",
    "pred_yad_df = pred_yad_df.fillna(10095).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred_yad_df.shape[0] == sample_submission.shape[0]\n",
    "assert list(pred_yad_df.columns) == list(sample_submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yad_df.to_csv(\n",
    "    f\"../sub/{NOTEBOOK_NAME}_auc{oof_score:.4f}_mapk{oof_mapk_score:.4f}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"../sub/{NOTEBOOK_NAME}_auc{oof_score:.4f}_mapk{oof_mapk_score:.4f}_dep_minus1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 人気の情報を入れて、CVが改善しているのにtestのスコアが改善しないのはおかしいため、原因を調べる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"e033_use_kyouki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    OUTPUT_DIR = f\"../saved_data/{NOTEBOOK_NAME}\"\n",
    "    SEED = 33\n",
    "    TARGET_COL = \"reserve\"\n",
    "\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = pd.read_csv(\"../data/train_log.csv\")\n",
    "train_label = pd.read_csv(\"../data/train_label.csv\")\n",
    "\n",
    "test_log = pd.read_csv(\"../data/test_log.csv\")\n",
    "test_session = pd.read_csv(\"../data/test_session.csv\")\n",
    "\n",
    "yado = pd.read_csv(\"../data/yado.csv\")\n",
    "\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "# # image_embeddings = pd.read_parquet(\"../data/image_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001149e9c73985425197104712478c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000e02747d749a52b7736dfa751e258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000f17ae2628237d78d3a38b009d3be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000174a6f7a569b84c5575760d2e9664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017e2a527901c9c41b1acef525d016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174695</th>\n",
       "      <td>fffee3199ef94b92283239cd5e3534fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174696</th>\n",
       "      <td>ffff62c6bb49bc9c0fbcf08494a4869c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174697</th>\n",
       "      <td>ffff9a7dcc892875c7a8b821fa436228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174698</th>\n",
       "      <td>ffffb1d30300fe17f661941fd085b04b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174699</th>\n",
       "      <td>ffffe984aafd6127ce8e43e3ca40c79d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174700 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              session_id\n",
       "0       00001149e9c73985425197104712478c\n",
       "1       0000e02747d749a52b7736dfa751e258\n",
       "2       0000f17ae2628237d78d3a38b009d3be\n",
       "3       000174a6f7a569b84c5575760d2e9664\n",
       "4       00017e2a527901c9c41b1acef525d016\n",
       "...                                  ...\n",
       "174695  fffee3199ef94b92283239cd5e3534fa\n",
       "174696  ffff62c6bb49bc9c0fbcf08494a4869c\n",
       "174697  ffff9a7dcc892875c7a8b821fa436228\n",
       "174698  ffffb1d30300fe17f661941fd085b04b\n",
       "174699  ffffe984aafd6127ce8e43e3ca40c79d\n",
       "\n",
       "[174700 rows x 1 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\n",
    "    \"../saved_data/e026_make_data_zakopuro_baseline/train_candidate.parquet\"\n",
    ")\n",
    "\n",
    "test = pd.read_parquet(\n",
    "    \"../saved_data/e026_make_data_zakopuro_baseline/test_candidate.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173861"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"session_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserveを付与する\n",
    "# 正解ラベルに含まれているレコードの index を配列で取得して\n",
    "target_index = pd.merge(\n",
    "    train.reset_index(), train_label, on=[\"session_id\", \"yad_no\"], how=\"inner\"\n",
    ")[\"index\"].values\n",
    "\n",
    "# 正解Indexに含まれている場合 1 / そうでないと 0 のラベルを作成\n",
    "train[\"reserve\"] = train.index.isin(target_index).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# train = train.sample(10000, random_state=Config.SEED).reset_index(drop=True)\n",
    "# test = test.sample(10000, random_state=Config.SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug用\n",
    "# train = train.sample(10000, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ひとつ前のlogのyad_idの値を取得\n",
    "# train[\"previous_1_yad_no\"] = train[\"logged_yad_no_list\"].apply(lambda x: x[-1])\n",
    "# test[\"previous_1_yad_no\"] = test[\"logged_yad_no_list\"].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sessionの情報を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_length(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionの長さを追加する\n",
    "    \"\"\"\n",
    "    train_session_id_cnt_dict = (\n",
    "        train_log.groupby(\"session_id\")[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "    test_session_id_cnt_dict = (\n",
    "        test_log.groupby(\"session_id\")[\"yad_no\"].count().to_dict()\n",
    "    )\n",
    "\n",
    "    train[\"session_length\"] = train[\"session_id\"].map(train_session_id_cnt_dict)\n",
    "    test[\"session_length\"] = test[\"session_id\"].map(test_session_id_cnt_dict)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_length(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001149e9c73985425197104712478c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000e02747d749a52b7736dfa751e258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000f17ae2628237d78d3a38b009d3be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000174a6f7a569b84c5575760d2e9664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017e2a527901c9c41b1acef525d016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174695</th>\n",
       "      <td>fffee3199ef94b92283239cd5e3534fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174696</th>\n",
       "      <td>ffff62c6bb49bc9c0fbcf08494a4869c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174697</th>\n",
       "      <td>ffff9a7dcc892875c7a8b821fa436228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174698</th>\n",
       "      <td>ffffb1d30300fe17f661941fd085b04b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174699</th>\n",
       "      <td>ffffe984aafd6127ce8e43e3ca40c79d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174700 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              session_id\n",
       "0       00001149e9c73985425197104712478c\n",
       "1       0000e02747d749a52b7736dfa751e258\n",
       "2       0000f17ae2628237d78d3a38b009d3be\n",
       "3       000174a6f7a569b84c5575760d2e9664\n",
       "4       00017e2a527901c9c41b1acef525d016\n",
       "...                                  ...\n",
       "174695  fffee3199ef94b92283239cd5e3534fa\n",
       "174696  ffff62c6bb49bc9c0fbcf08494a4869c\n",
       "174697  ffff9a7dcc892875c7a8b821fa436228\n",
       "174698  ffffb1d30300fe17f661941fd085b04b\n",
       "174699  ffffe984aafd6127ce8e43e3ca40c79d\n",
       "\n",
       "[174700 rows x 1 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>yad_no</th>\n",
       "      <th>session_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005ab22bbcba87c59b4a6718fd5709a6</td>\n",
       "      <td>13550</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00a6564cf7ae33af02c9ecfeac0a0b07</td>\n",
       "      <td>635</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00ef7a2cd4ecf01f1921cfff1dc606dc</td>\n",
       "      <td>4913</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0116b42aeb1e24c9d9796b9908879ebe</td>\n",
       "      <td>5415</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>012bffe33b46e2c19355af73a6f9dee8</td>\n",
       "      <td>4289</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589420</th>\n",
       "      <td>c9390440cfbfd87a870841f34fa24315</td>\n",
       "      <td>10682</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589421</th>\n",
       "      <td>e579e692d35ebeefefe270738b4cab5e</td>\n",
       "      <td>6467</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589422</th>\n",
       "      <td>e579e692d35ebeefefe270738b4cab5e</td>\n",
       "      <td>11389</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589423</th>\n",
       "      <td>bf33b07e9f70817b1b11253aa343819e</td>\n",
       "      <td>7832</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589424</th>\n",
       "      <td>bf33b07e9f70817b1b11253aa343819e</td>\n",
       "      <td>4588</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1589425 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               session_id  yad_no  session_length\n",
       "0        005ab22bbcba87c59b4a6718fd5709a6   13550               2\n",
       "1        00a6564cf7ae33af02c9ecfeac0a0b07     635               4\n",
       "2        00ef7a2cd4ecf01f1921cfff1dc606dc    4913               3\n",
       "3        0116b42aeb1e24c9d9796b9908879ebe    5415               2\n",
       "4        012bffe33b46e2c19355af73a6f9dee8    4289               2\n",
       "...                                   ...     ...             ...\n",
       "1589420  c9390440cfbfd87a870841f34fa24315   10682               1\n",
       "1589421  e579e692d35ebeefefe270738b4cab5e    6467               2\n",
       "1589422  e579e692d35ebeefefe270738b4cab5e   11389               2\n",
       "1589423  bf33b07e9f70817b1b11253aa343819e    7832               2\n",
       "1589424  bf33b07e9f70817b1b11253aa343819e    4588               2\n",
       "\n",
       "[1589425 rows x 3 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_yado_nunique(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionの中で登場したyadoの数を追加する\n",
    "    \"\"\"\n",
    "    train_session_yado_cnt_dict = (\n",
    "        train_log.groupby(\"session_id\")[\"yad_no\"].nunique().to_dict()\n",
    "    )\n",
    "    test_session_yado_cnt_dict = (\n",
    "        test_log.groupby(\"session_id\")[\"yad_no\"].nunique().to_dict()\n",
    "    )\n",
    "\n",
    "    train[\"session_yado_nunique\"] = train[\"session_id\"].map(train_session_yado_cnt_dict)\n",
    "    test[\"session_yado_nunique\"] = test[\"session_id\"].map(test_session_yado_cnt_dict)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_yado_nunique(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_session_yado_cnt(\n",
    "    train: pd.DataFrame,\n",
    "    train_log: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    test_log: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sessionとyadごとに、yad_noが登場した回数を数える\n",
    "    \"\"\"\n",
    "    train = train.copy()\n",
    "    train_log = train_log.copy()\n",
    "    test = test.copy()\n",
    "    test_log = test_log.copy()\n",
    "\n",
    "    train_session_yado_cnt = (\n",
    "        train_log.groupby([\"session_id\", \"yad_no\"])[\"yad_no\"]\n",
    "        .count()\n",
    "        .rename(\"session_yado_cnt\")\n",
    "    )\n",
    "    test_session_yado_cnt = (\n",
    "        test_log.groupby([\"session_id\", \"yad_no\"])[\"yad_no\"]\n",
    "        .count()\n",
    "        .rename(\"session_yado_cnt\")\n",
    "    )\n",
    "\n",
    "    train = train.merge(train_session_yado_cnt, on=[\"session_id\", \"yad_no\"], how=\"left\")\n",
    "    train[\"session_yado_cnt\"] = train[\"session_yado_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "    test = test.merge(test_session_yado_cnt, on=[\"session_id\", \"yad_no\"], how=\"left\")\n",
    "    test[\"session_yado_cnt\"] = test[\"session_yado_cnt\"].fillna(0).astype(int)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_session_yado_cnt(train, train_log, test, test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、宿の情報の統計値（平均、最大、最小、分散、中央値）をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[240], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m     agg_col_name \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myad_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magg_way\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yad_col \u001b[38;5;129;01min\u001b[39;00m use_cols \u001b[38;5;28;01mfor\u001b[39;00m agg_way \u001b[38;5;129;01min\u001b[39;00m agg_ways\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session_df, agg_col_name\n\u001b[0;32m---> 38\u001b[0m train, yado_agg_col \u001b[38;5;241m=\u001b[39m \u001b[43madd_yad_statistic_from_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myado\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m test, _ \u001b[38;5;241m=\u001b[39m add_yad_statistic_from_log(test_log, test, yado)\n",
      "Cell \u001b[0;32mIn[240], line 29\u001b[0m, in \u001b[0;36madd_yad_statistic_from_log\u001b[0;34m(log_df, session_df, yado)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m yad_col \u001b[38;5;129;01min\u001b[39;00m use_cols:\n\u001b[1;32m     24\u001b[0m     agg_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     25\u001b[0m         log_with_yad\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)[yad_col]\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;241m.\u001b[39magg(agg_ways)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;241m.\u001b[39madd_prefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myad_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m     session_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m agg_col_name \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myad_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magg_way\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m yad_col \u001b[38;5;129;01min\u001b[39;00m use_cols \u001b[38;5;28;01mfor\u001b[39;00m agg_way \u001b[38;5;129;01min\u001b[39;00m agg_ways\n\u001b[1;32m     33\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session_df, agg_col_name\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:162\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    148\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    149\u001b[0m         left,\n\u001b[1;32m    150\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:809\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m--> 809\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[1;32m    812\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    813\u001b[0m )\n\u001b[1;32m    814\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1065\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[1;32m   1062\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[1;32m   1063\u001b[0m     )\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1065\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1038\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1665\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;66;03m# get left & right join labels and num. of levels at each location\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m mapped \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1662\u001b[0m     _factorize_keys(left_keys[n], right_keys[n], sort\u001b[38;5;241m=\u001b[39msort, how\u001b[38;5;241m=\u001b[39mhow)\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(left_keys))\n\u001b[1;32m   1664\u001b[0m )\n\u001b[0;32m-> 1665\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1666\u001b[0m llab, rlab, shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m zipped)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# get flat i8 keys from label lists\u001b[39;00m\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1662\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _get_no_sort_one_missing_indexer(left_n, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;66;03m# get left & right join labels and num. of levels at each location\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m mapped \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1662\u001b[0m     \u001b[43m_factorize_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(left_keys))\n\u001b[1;32m   1664\u001b[0m )\n\u001b[1;32m   1665\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mmapped)\n\u001b[1;32m   1666\u001b[0m llab, rlab, shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m zipped)\n",
      "File \u001b[0;32m~/atmacup16/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:2443\u001b[0m, in \u001b[0;36m_factorize_keys\u001b[0;34m(lk, rk, sort, how)\u001b[0m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2439\u001b[0m     \u001b[38;5;66;03m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   2440\u001b[0m     \u001b[38;5;66;03m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;66;03m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[1;32m   2442\u001b[0m     llab \u001b[38;5;241m=\u001b[39m rizer\u001b[38;5;241m.\u001b[39mfactorize(lk)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 2443\u001b[0m     rlab \u001b[38;5;241m=\u001b[39m \u001b[43mrizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2444\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m llab\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mintp), llab\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   2445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rlab\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mintp), rlab\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def add_yad_statistic_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\n",
    "        \"yad_type\",\n",
    "        \"total_room_cnt\",\n",
    "        \"wireless_lan_flg\",\n",
    "        \"onsen_flg\",\n",
    "        \"kd_stn_5min\",\n",
    "        \"kd_bch_5min\",\n",
    "        \"kd_slp_5min\",\n",
    "        \"kd_conv_walk_5min\",\n",
    "    ]\n",
    "\n",
    "    agg_ways = [\"mean\", \"max\", \"min\", \"std\", \"median\"]\n",
    "\n",
    "    for yad_col in use_cols:\n",
    "        agg_df = (\n",
    "            log_with_yad.groupby(\"session_id\")[yad_col]\n",
    "            .agg(agg_ways)\n",
    "            .add_prefix(f\"{yad_col}_\")\n",
    "        )\n",
    "        session_df = pd.merge(session_df, agg_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    agg_col_name = [\n",
    "        f\"{yad_col}_{agg_way}\" for yad_col in use_cols for agg_way in agg_ways\n",
    "    ]\n",
    "\n",
    "    return session_df, agg_col_name\n",
    "\n",
    "\n",
    "train, yado_agg_col = add_yad_statistic_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_statistic_from_log(test_log, test, yado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logの中で、wid_cd、ken_cd、lrg_cd、sml_cdのnuniqueをとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yad_area_nunique_from_log(\n",
    "    log_df: pd.DataFrame, session_df: pd.DataFrame, yado: pd.DataFrame\n",
    "):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "    yado = yado.copy()\n",
    "\n",
    "    log_with_yad = pd.merge(log_df, yado, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    use_cols = [\"wid_cd\", \"ken_cd\", \"lrg_cd\", \"sml_cd\"]\n",
    "\n",
    "    for area_col in use_cols:\n",
    "        agg_df = (\n",
    "            log_with_yad.groupby(\"session_id\")[area_col]\n",
    "            .nunique()\n",
    "            .rename(f\"{area_col}_nunique\")\n",
    "        )\n",
    "        session_df = session_df.merge(agg_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    out_cols = [f\"{area_col}_nunique\" for area_col in use_cols]\n",
    "\n",
    "    return session_df, out_cols\n",
    "\n",
    "\n",
    "train, yad_area_nunique = add_yad_area_nunique_from_log(train_log, train, yado)\n",
    "test, _ = add_yad_area_nunique_from_log(train_log, test, yado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session lengthが2以上の場合は、該当の宿がreverse_seq_no(max_seq_no - seq_no)を追加し、reverse_seq_noが偶数か奇数かのフラグを立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reverse_seq_no(log_df: pd.DataFrame, session_df: pd.DataFrame):\n",
    "    log_df = log_df.copy()\n",
    "    session_df = session_df.copy()\n",
    "\n",
    "    # session_dfにseq_noを結合する\n",
    "    log_df_no_dup = log_df.drop_duplicates(subset=[\"session_id\", \"yad_no\"], keep=\"last\")\n",
    "    session_df = session_df.merge(\n",
    "        log_df_no_dup, on=[\"session_id\", \"yad_no\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # sessionごとに最大のseq_noを結合する\n",
    "    log_max_seq_no = log_df.groupby(\"session_id\")[\"seq_no\"].max()\n",
    "    log_max_seq_no.name = \"max_seq_no\"\n",
    "    session_df = session_df.merge(log_max_seq_no, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # sessionの最大のseq_noの差分を取る(そのセッションが最後から何番目か？)\n",
    "    session_df[\"reverse_seq_no\"] = session_df[\"max_seq_no\"] - session_df[\"seq_no\"]\n",
    "    session_df[\"is_reverse_seq_no_odd\"] = session_df[\"reverse_seq_no\"] % 2\n",
    "\n",
    "    session_df.drop(columns=[\"seq_no\", \"max_seq_no\"], inplace=True)\n",
    "\n",
    "    return session_df\n",
    "\n",
    "\n",
    "train = add_reverse_seq_no(log_df=train_log, session_df=train)\n",
    "test = add_reverse_seq_no(log_df=test_log, session_df=test)\n",
    "# TODO: ここの処理が正しいか確認する\n",
    "# -> OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_seq_feat = [\"reverse_seq_no\", \"is_reverse_seq_no_odd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正解のyadoと1つ前のyadoの情報の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一番直近に見たyad_noを追加\n",
    "train_previous_1_yad_no_dict = (\n",
    "    train_log.groupby(\"session_id\")[\"yad_no\"].apply(lambda x: list(x)[-1]).to_dict()\n",
    ")\n",
    "test_previous_1_yad_no_dict = (\n",
    "    test_log.groupby(\"session_id\")[\"yad_no\"].apply(lambda x: list(x)[-1]).to_dict()\n",
    ")\n",
    "\n",
    "train[\"previous_1_yad_no\"] = train[\"session_id\"].map(train_previous_1_yad_no_dict)\n",
    "test[\"previous_1_yad_no\"] = test[\"session_id\"].map(test_previous_1_yad_no_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解のyado情報の追加\n",
    "train = pd.merge(\n",
    "    train,\n",
    "    yado.add_prefix(\"now_\"),\n",
    "    left_on=\"yad_no\",\n",
    "    right_on=\"now_yad_no\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 1つ前のyado情報の追加\n",
    "train = pd.merge(\n",
    "    train,\n",
    "    yado.add_prefix(\"previous_1_\"),\n",
    "    on=\"previous_1_yad_no\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解のyado情報の追加\n",
    "test = pd.merge(\n",
    "    test,\n",
    "    yado.add_prefix(\"now_\"),\n",
    "    left_on=\"yad_no\",\n",
    "    right_on=\"now_yad_no\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 1つ前のyado情報の追加\n",
    "test = pd.merge(\n",
    "    test,\n",
    "    yado.add_prefix(\"previous_1_\"),\n",
    "    on=\"previous_1_yad_no\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今の部屋と、前の部屋のwid_cd、ken_cd、lrg_cd、sml_cdが一緒かどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"same_wid_cd_now_previous_1\"] = train[\"now_wid_cd\"] == train[\"previous_1_wid_cd\"]\n",
    "train[\"same_ken_cd_now_previous_1\"] = train[\"now_ken_cd\"] == train[\"previous_1_ken_cd\"]\n",
    "train[\"same_lrg_cd_now_previous_1\"] = train[\"now_lrg_cd\"] == train[\"previous_1_lrg_cd\"]\n",
    "train[\"same_sml_cd_now_previous_1\"] = train[\"now_sml_cd\"] == train[\"previous_1_sml_cd\"]\n",
    "\n",
    "test[\"same_wid_cd_now_previous_1\"] = test[\"now_wid_cd\"] == test[\"previous_1_wid_cd\"]\n",
    "test[\"same_ken_cd_now_previous_1\"] = test[\"now_ken_cd\"] == test[\"previous_1_ken_cd\"]\n",
    "test[\"same_lrg_cd_now_previous_1\"] = test[\"now_lrg_cd\"] == test[\"previous_1_lrg_cd\"]\n",
    "test[\"same_sml_cd_now_previous_1\"] = test[\"now_sml_cd\"] == test[\"previous_1_sml_cd\"]\n",
    "\n",
    "same_area_feat = [\n",
    "    \"same_wid_cd_now_previous_1\",\n",
    "    \"same_ken_cd_now_previous_1\",\n",
    "    \"same_lrg_cd_now_previous_1\",\n",
    "    \"same_sml_cd_now_previous_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foldごとに、rulebaseで推論した結果をrulebeased_predict_yadoとして追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted_df = pd.read_pickle(\n",
    "    \"../saved_data/e024_make_rulebased_feat/all_rulebased_predict_df_train.pkl\"\n",
    ")\n",
    "\n",
    "test_predicted_df = pd.read_pickle(\n",
    "    \"../saved_data/e024_make_rulebased_feat/all_rulebased_predict_df_test.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_predicted_df, on=\"session_id\", how=\"left\")\n",
    "test = pd.merge(test, test_predicted_df, on=\"session_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulebased_predict_feat = [\n",
    "    \"rulebased_predict_0\",\n",
    "    \"rulebased_predict_1\",\n",
    "    \"rulebased_predict_2\",\n",
    "    \"rulebased_predict_3\",\n",
    "    \"rulebased_predict_4\",\n",
    "    \"rulebased_predict_5\",\n",
    "    \"rulebased_predict_6\",\n",
    "    \"rulebased_predict_7\",\n",
    "    \"rulebased_predict_8\",\n",
    "    \"rulebased_predict_9\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train[f\"same_rulebased_predict_{i}\"] = (\n",
    "        train[\"yad_no\"] == train[f\"rulebased_predict_{i}\"]\n",
    "    )\n",
    "    test[f\"same_rulebased_predict_{i}\"] = (\n",
    "        test[\"yad_no\"] == test[f\"rulebased_predict_{i}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_rulebased_predict_feat = [f\"same_rulebased_predict_{i}\" for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ型の変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_colの定義\n",
    "base_categorical_cols = [\n",
    "    \"yad_no\",\n",
    "    \"yad_type\",\n",
    "    \"wid_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"ken_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"lrg_cd\",  # retrieveの条件が同じlrg_cdのデータの予定のため、今は学習に使わない\n",
    "    \"sml_cd\",\n",
    "]\n",
    "now_yado_categorical_cols = [f\"now_{col}\" for col in base_categorical_cols]\n",
    "previous_1_yado_categorical_cols = [\n",
    "    f\"previous_1_{col}\" for col in base_categorical_cols\n",
    "]\n",
    "\n",
    "categorical_cols = (\n",
    "    now_yado_categorical_cols\n",
    "    + previous_1_yado_categorical_cols\n",
    "    + rulebased_predict_feat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    train[col] = train[col].astype(\"category\")\n",
    "    test[col] = test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cross Validationの際に用いるfold(分割する際のグループ番号)を追加\n",
    "# FOLD_NUM = 5\n",
    "\n",
    "# # skf = StratifiedKFold(n_splits=FOLD_NUM, shuffle=True, random_state=42)\n",
    "# # for fold, (_, v_idx) in enumerate(\n",
    "# #     skf.split(train, pd.cut(train[\"reserve\"], bins=3, labels=[\"0\", \"0.5\", \"1\"]))\n",
    "# # ):\n",
    "# #     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# # TODO: yをsession_lengthにして試してみる\n",
    "# # sgkf = StratifiedGroupKFold(n_splits=FOLD_NUM, shuffle=True, random_state=Config.SEED)\n",
    "# # for fold, (_, v_idx) in enumerate(\n",
    "# #     sgkf.split(\n",
    "# #         X=train,\n",
    "# #         y=train[\"reserve\"],\n",
    "# #         groups=train[\"session_id\"],\n",
    "# #     )\n",
    "# # ):\n",
    "# #     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# gkf = GroupKFold(n_splits=FOLD_NUM)\n",
    "# for fold, (_, v_idx) in enumerate(\n",
    "#     gkf.split(\n",
    "#         X=train,\n",
    "#         groups=train[\"session_id\"],\n",
    "#     )\n",
    "# ):\n",
    "#     train.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "# train[\"fold\"] = train[\"fold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e016にて、データ作成時にfoldを利用するように変更\n",
    "with open(\n",
    "    \"../saved_data/e016_make_train_popular_base/session_id_fold_dict.pkl\", \"rb\"\n",
    ") as f:\n",
    "    session_id_fold_dict = pickle.load(f)\n",
    "\n",
    "train[\"fold\"] = train[\"session_id\"].map(session_id_fold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"fold\")[\"reserve\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人気の宿情報、つまり予約された回数をfoldごとに付与する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_fold_df = train[[\"session_id\", \"fold\"]].drop_duplicates()\n",
    "session_id_fold_dict = dict(\n",
    "    zip(session_id_fold_df[\"session_id\"], session_id_fold_df[\"fold\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_popular_per_fold(\n",
    "    train: pd.DataFrame,\n",
    "    train_label: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    n_fold: int = FOLD_NUM,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    foldごとに、宿が予約された回数を計算する\n",
    "    NOTE: 人気情報を付与して0.42ぐらいかなかったら何か間違っていそう\n",
    "    \"\"\"\n",
    "    train_label_copy = train_label.copy()\n",
    "\n",
    "    # train_labelに対して、trainを用いてsession_idごとのfoldを付与\n",
    "    session_id_fold_df = train[[\"session_id\", \"fold\"]].drop_duplicates()\n",
    "    session_id_fold_dict = dict(\n",
    "        zip(session_id_fold_df[\"session_id\"], session_id_fold_df[\"fold\"])\n",
    "    )\n",
    "\n",
    "    # その後、train_labelを用いて、foldごとにreserveの合計を計算\n",
    "    train_label_copy[\"fold\"] = train_label_copy[\"session_id\"].map(session_id_fold_dict)\n",
    "\n",
    "    # 学習データへの人気宿情報の付与\n",
    "    for fold in range(n_fold):\n",
    "        train_out_of_fold_df = train_label_copy[train_label_copy[\"fold\"] != fold]\n",
    "\n",
    "        train_yad_no_cnt_per_fold_dict = (\n",
    "            train_out_of_fold_df.groupby(\"yad_no\")[\"yad_no\"].count()\n",
    "            / train_out_of_fold_df.shape[0]\n",
    "        ).to_dict()\n",
    "        train.loc[train[\"fold\"] == fold, \"popular_yado_rate_per_fold\"] = train.loc[\n",
    "            train[\"fold\"] == fold, \"yad_no\"\n",
    "        ].map(train_yad_no_cnt_per_fold_dict)\n",
    "\n",
    "    # テストデータへの人気宿情報の付与\n",
    "    test_yad_no_cnt_per_fold_dict = (\n",
    "        train_label_copy.groupby(\"yad_no\")[\"yad_no\"].count() / train_label_copy.shape[0]\n",
    "    ).to_dict()\n",
    "    test[\"popular_yado_rate_per_fold\"] = test[\"yad_no\"].map(\n",
    "        test_yad_no_cnt_per_fold_dict\n",
    "    )\n",
    "\n",
    "    # 学習・テスト共に、1度も登場しなかった宿は予約回数が0回となるため、0に置換\n",
    "    train[\"popular_yado_rate_per_fold\"] = train[\"popular_yado_rate_per_fold\"].fillna(0)\n",
    "    test[\"popular_yado_rate_per_fold\"] = test[\"popular_yado_rate_per_fold\"].fillna(0)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = add_popular_per_fold(train, train_label, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共起の情報を追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyouki_df_train = pd.read_pickle(\n",
    "    f\"../saved_data/e033_make_feat_kyouki/kyouki_df_train.pkl\"\n",
    ")\n",
    "kyouki_df_test = pd.read_pickle(\n",
    "    f\"../saved_data/e033_make_feat_kyouki/kyouki_df_test.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyouki_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(\n",
    "    kyouki_df_train,\n",
    "    left_on=[\"fold\", \"previous_1_yad_no\"],\n",
    "    right_on=[\"fold\", \"latest_yad_no\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "test = test.merge(\n",
    "    kyouki_df_test,\n",
    "    left_on=\"previous_1_yad_no\",\n",
    "    right_on=\"latest_yad_no\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyouki_feat = [f\"kyouki_arr_reduced_{col}\" for col in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習とテストに使うデータを保存する\n",
    "train.to_pickle(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_train.pkl\")\n",
    "\n",
    "test.to_pickle(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_colの定義\n",
    "base_numeric_col = [\n",
    "    \"total_room_cnt\",\n",
    "    \"wireless_lan_flg\",\n",
    "    \"onsen_flg\",\n",
    "    \"kd_stn_5min\",\n",
    "    \"kd_bch_5min\",\n",
    "    \"kd_slp_5min\",\n",
    "    \"kd_conv_walk_5min\",\n",
    "]\n",
    "\n",
    "now_yado_numeric_col = [f\"now_{col}\" for col in base_numeric_col]\n",
    "previous_1_yado_numeric_col = [f\"previous_1_{col}\" for col in base_numeric_col]\n",
    "\n",
    "# session系の特徴量\n",
    "session_numeric_col = [\n",
    "    \"session_length\",\n",
    "    \"session_yado_nunique\",\n",
    "    \"session_yado_cnt\",\n",
    "]\n",
    "\n",
    "per_fold_col = [\"popular_yado_rate_per_fold\"]\n",
    "\n",
    "# numeric_cols = now_yado_numeric_col + previous_1_yado_numeric_col\n",
    "# numeric_cols = now_yado_numeric_col\n",
    "numeric_cols = (\n",
    "    now_yado_numeric_col\n",
    "    + previous_1_yado_numeric_col\n",
    "    + session_numeric_col\n",
    "    + per_fold_col\n",
    "    + same_area_feat  # 同じエリアかどうか\n",
    "    + yado_agg_col  # yadoの統計量\n",
    "    + yad_area_nunique  # areaの統計量\n",
    "    + reverse_seq_feat  # 逆順の特徴量\n",
    "    + kyouki_feat  # 共起特徴量\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_col = numeric_cols + categorical_cols\n",
    "len(use_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで用いるパラメーターを指定\n",
    "# ref: https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"max_depth\": -1,\n",
    "    # \"min_data_in_leaf\": 100, # 1つの葉に入る最小のデータ数\n",
    "    \"num_leaves\": 24,  # 2**max_depthより少し小さめにすると過学習を防げる\n",
    "    \"learning_rate\": 0.05,  # 1回のiterationで学習を進める割合、大きいと学習が早く終わる。小さいと学習は長いが高精度になりやすい。\n",
    "    \"bagging_freq\": 5,  # 指定した回数ごとにbaggingを行う\n",
    "    \"feature_fraction\": 0.9,  # 1回のiterationで利用する特徴量(列方向)の割合\n",
    "    \"bagging_fraction\": 0.8,  # 1回のiterationで利用するデータ(行方向)の割合\n",
    "    \"verbose\": -1,  # 出力するログレベルの変更、0はError(Warning)以上を表示\n",
    "    \"seed\": 42,  # ランダムシードの固定\n",
    "    \"lambda_l1\": 0.4,\n",
    "    \"lambda_l2\": 0.4,\n",
    "    \"importance_type\": \"gain\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     # 目的関数. これの意味で最小となるようなパラメータを探します.\n",
    "#     \"objective\": \"binary\",\n",
    "#     # 木の最大数\n",
    "#     \"n_estimators\": 10000,\n",
    "#     # 学習率. 小さいほどなめらかな決定境界が作られて性能向上に繋がる場合が多いです、\n",
    "#     # がそれだけ木を作るため学習に時間がかかります\n",
    "#     # 今回設定している 0.3 は比較的大きめの設定です\n",
    "#     \"learning_rate\": 0.3,\n",
    "#     # 特徴重要度計算のロジック(後述)\n",
    "#     \"importance_type\": \"gain\",\n",
    "#     \"random_state\": 510,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rmse(y_true, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データ全体に対する推論結果を保存するobjectを作成\n",
    "oof = np.zeros((len(train)))\n",
    "\n",
    "# テストデータに対する推論、特徴量重要度(後述)を計算するために、モデルを保存するobjectを作成\n",
    "models = []\n",
    "\n",
    "# Cross Validationによる学習の実施\n",
    "for fold in range(FOLD_NUM):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # foldごとにtrainとvalidに分ける\n",
    "    train_fold = train[train[\"fold\"] != fold]\n",
    "    valid_fold = train[train[\"fold\"] == fold]\n",
    "\n",
    "    # X(説明変数)とy(目的変数)に分ける\n",
    "    X_train = train_fold.drop(Config.TARGET_COL, axis=1)\n",
    "    X_valid = valid_fold.drop(Config.TARGET_COL, axis=1)\n",
    "    y_train = train_fold[[Config.TARGET_COL]]\n",
    "    y_valid = valid_fold[[Config.TARGET_COL]]\n",
    "\n",
    "    # 利用する説明変数に限定する\n",
    "    X_train = X_train[use_col]\n",
    "    X_valid = X_valid[use_col]\n",
    "\n",
    "    # LightGBMが認識可能な形にデータセットを変換\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "    # モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=2000,  # 学習のiteration回数\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(100),\n",
    "        ],  # Early stopingの回数、binary_loglossが改善しないiterationが100回続いたら学習を止める\n",
    "    )\n",
    "\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "\n",
    "    # validデータに対する推論\n",
    "    y_valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "    # validデータに対する推論の性能を計算\n",
    "    # score = rmse(y_valid, y_valid_pred)\n",
    "    score = roc_auc_score(y_valid, y_valid_pred)\n",
    "\n",
    "    print(f\"fold {fold} Score: {score}\")\n",
    "\n",
    "    # oofに推論結果を保存\n",
    "    valid_idx = X_valid.index\n",
    "    oof[valid_idx] = y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_oof.pkl\", \"wb\") as f:\n",
    "    pickle.dump(oof, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_models.pkl\", \"rb\") as f:\n",
    "    models = pickle.load(f)\n",
    "\n",
    "with open(f\"{Config.OUTPUT_DIR}/{NOTEBOOK_NAME}_oof.pkl\", \"rb\") as f:\n",
    "    oof = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_score = rmse(train[Config.TARGET_COL][oof != 0], oof[oof != 0])\n",
    "oof_score = roc_auc_score(train[Config.TARGET_COL][oof != 0], oof[oof != 0])\n",
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"oof_pred\"] = oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainをoofが0でないものだけに絞る\n",
    "def get_oof_pred_df(train: pd.DataFrame, oof: np.ndarray) -> pd.DataFrame:\n",
    "    train_for_calc_mapk = train.copy()\n",
    "    train_for_calc_mapk[\"oof\"] = oof\n",
    "    train_for_calc_mapk = train_for_calc_mapk[train_for_calc_mapk[\"oof\"] != 0]\n",
    "\n",
    "    # 推論順にsession_idとyad_noを並べる\n",
    "    oof_pred_yad = (\n",
    "        train_for_calc_mapk.sort_values([\"session_id\", \"oof\"], ascending=False)\n",
    "        .groupby(\"session_id\")[\"yad_no\"]\n",
    "        .apply(list)\n",
    "    ).to_dict()\n",
    "\n",
    "    # train_labelをoofの計算用に用意\n",
    "    train_label_for_calc_oof = train_label.copy()\n",
    "\n",
    "    # train_for_calc_mapkに付与\n",
    "    train_label_for_calc_oof[\"pred_yad_no_list\"] = train_label_for_calc_oof[\n",
    "        \"session_id\"\n",
    "    ].map(oof_pred_yad)\n",
    "\n",
    "    # oofが計算されていないsession_idは削除\n",
    "    train_label_for_calc_oof = train_label_for_calc_oof[\n",
    "        train_label_for_calc_oof[\"pred_yad_no_list\"].notnull()\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # 上位10件に限定\n",
    "    train_label_for_calc_oof[\"pred_yad_no_list_top10\"] = train_label_for_calc_oof[\n",
    "        \"pred_yad_no_list\"\n",
    "    ].apply(lambda x: x[:10])\n",
    "\n",
    "    # listをpd.Seriesに変換\n",
    "    oof_pred_df = train_label_for_calc_oof.set_index(\"session_id\")[\n",
    "        \"pred_yad_no_list_top10\"\n",
    "    ].apply(pd.Series)\n",
    "    oof_pred_df = oof_pred_df.rename(columns=lambda x: \"predict_\" + str(x))\n",
    "\n",
    "    # Nullの箇所はyad_no=0で保管し、全ての値をintに変換する\n",
    "    # TODO: 埋めるのは0で本当に良いのか考える\n",
    "    oof_pred_df = oof_pred_df.fillna(0).astype(int)\n",
    "\n",
    "    return oof_pred_df\n",
    "\n",
    "\n",
    "oof_pred_df = get_oof_pred_df(train, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label[train_label[\"session_id\"].isin(oof_pred_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"0000ca043ed437a1472c9d1d154eb49b\"\n",
    "train_log[train_log[\"session_id\"] == session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"session_id\"] == session_id].to_csv(\"aa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train[\"previous_1_yad_no\"] == 13535) & (train[\"reserve\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single actual value.\n",
    "\n",
    "    Parameters:\n",
    "    actual : int\n",
    "        The actual value that is to be predicted\n",
    "    predicted : list\n",
    "        A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The average precision at k\n",
    "    \"\"\"\n",
    "    if actual in predicted[:k]:\n",
    "        return 1.0 / (predicted[:k].index(actual) + 1)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k for lists of actual values and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    actual : list\n",
    "        A list of actual values that are to be predicted\n",
    "    predicted : list\n",
    "        A list of lists of predicted elements (order does matter in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The mean average precision at k\n",
    "    \"\"\"\n",
    "    return sum(apk(a, p, k) for a, p in zip(actual, predicted)) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPK (k=10) として計算\n",
    "sorted_train_label = (\n",
    "    train_label[train_label[\"session_id\"].isin(oof_pred_df.index)]\n",
    "    .sort_values(\"session_id\")[\"yad_no\"]\n",
    "    .values\n",
    ")\n",
    "\n",
    "assert len(sorted_train_label) == len(oof_pred_df)\n",
    "\n",
    "oof_mapk_score = mapk(\n",
    "    actual=sorted_train_label,\n",
    "    predicted=oof_pred_df.sort_index().values.tolist(),\n",
    "    k=10,\n",
    ")\n",
    "oof_mapk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    _df = pd.DataFrame()\n",
    "    _df[f\"fold_{fold}\"] = model.feature_importance(importance_type=\"gain\")\n",
    "    _df = _df.T\n",
    "    _df.columns = use_col\n",
    "    feature_importance_df = pd.concat([feature_importance_df, _df], axis=0)\n",
    "order = _df.mean().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(6, len(order) * 0.4), len(order) * 0.5))\n",
    "sns.boxplot(\n",
    "    data=feature_importance_df, orient=\"h\", order=order, ax=ax, palette=\"viridis\"\n",
    ")\n",
    "ax.grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testに対する推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test[use_col]) for model in models])\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0)  # axis=0 なので shape の `k` が潰れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = 0.02\n",
    "# bins = np.linspace(0, 1, 0.1)\n",
    "ax.hist(pred, density=True, alpha=0.5, label=\"Test\")\n",
    "ax.hist(oof, density=True, alpha=0.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pred\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP10に並び替え\n",
    "# session_idごとにpredが高いyadoのlistを取得\n",
    "pred_yad = (\n",
    "    test.sort_values([\"session_id\", \"pred\"], ascending=False)\n",
    "    .groupby(\"session_id\")[\"yad_no\"]\n",
    "    .apply(list)\n",
    ").to_dict()\n",
    "\n",
    "test_session[\"pred_yad_no_list\"] = test_session[\"session_id\"].map(pred_yad)\n",
    "\n",
    "# test_sessionのpred_yad_no_listがNaNの場合は、[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]で埋める\n",
    "test_session[\"pred_yad_no_list\"] = test_session[\"pred_yad_no_list\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    ")\n",
    "\n",
    "# 上位10件に限定\n",
    "test_session[\"pred_yad_no_list_top10\"] = test_session[\"pred_yad_no_list\"].apply(\n",
    "    lambda x: x[:10]\n",
    ")\n",
    "\n",
    "# listをpd.Seriesに変換\n",
    "pred_yad_df = test_session[\"pred_yad_no_list_top10\"].apply(pd.Series)\n",
    "pred_yad_df = pred_yad_df.rename(columns=lambda x: \"predict_\" + str(x))\n",
    "\n",
    "print(pred_yad_df.isnull().sum())\n",
    "\n",
    "# Nullの箇所はyad_no=10095(一番人気)で保管し、全ての値をintに変換する\n",
    "# NOTE: 保管するのは本当に10095で良いのか考える\n",
    "pred_yad_df = pred_yad_df.fillna(10095).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred_yad_df.shape[0] == sample_submission.shape[0]\n",
    "assert list(pred_yad_df.columns) == list(sample_submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yad_df.to_csv(\n",
    "    f\"../sub/{NOTEBOOK_NAME}_auc{oof_score:.4f}_mapk{oof_mapk_score:.4f}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"../sub/{NOTEBOOK_NAME}_auc{oof_score:.4f}_mapk{oof_mapk_score:.4f}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 人気の情報を入れて、CVが改善しているのにtestのスコアが改善しないのはおかしいため、原因を調べる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
